{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-18T12:17:19.043662Z","iopub.execute_input":"2022-04-18T12:17:19.044258Z","iopub.status.idle":"2022-04-18T12:17:19.077573Z","shell.execute_reply.started":"2022-04-18T12:17:19.044132Z","shell.execute_reply":"2022-04-18T12:17:19.076529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport os\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:19.138042Z","iopub.execute_input":"2022-04-18T12:17:19.138345Z","iopub.status.idle":"2022-04-18T12:17:19.143338Z","shell.execute_reply.started":"2022-04-18T12:17:19.138314Z","shell.execute_reply":"2022-04-18T12:17:19.14243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8주차 동안 배운 것들을 sklearn에서 모두 미리 불러옵니다. \n\nimport pandas_profiling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder \nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:19.221072Z","iopub.execute_input":"2022-04-18T12:17:19.221715Z","iopub.status.idle":"2022-04-18T12:17:22.763979Z","shell.execute_reply.started":"2022-04-18T12:17:19.22166Z","shell.execute_reply":"2022-04-18T12:17:22.76289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.read_csv('../input/2022-ml-midterm-p2/submit.csv')\ntest = pd.read_csv('../input/2022-ml-midterm-p2/test.csv')\ntrain = pd.read_csv('../input/2022-ml-midterm-p2/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:22.766015Z","iopub.execute_input":"2022-04-18T12:17:22.766277Z","iopub.status.idle":"2022-04-18T12:17:22.816183Z","shell.execute_reply.started":"2022-04-18T12:17:22.766249Z","shell.execute_reply":"2022-04-18T12:17:22.81511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainX, trainY 데이터를 만들어 줍니다.\n# 필요없는 열인 index를 없애줍니다.\ntrainX = train.drop(['index','label'], axis=1)\ntrainY = train['label']\ntestX = test.drop(['index'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:22.8174Z","iopub.execute_input":"2022-04-18T12:17:22.817611Z","iopub.status.idle":"2022-04-18T12:17:22.837315Z","shell.execute_reply.started":"2022-04-18T12:17:22.817585Z","shell.execute_reply":"2022-04-18T12:17:22.836137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [1] KNN 사용한 모델학습\nmodel = KNeighborsClassifier(n_neighbors=7, weights='distance')\nmodel.fit(trainX, trainY)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:22.839429Z","iopub.execute_input":"2022-04-18T12:17:22.839679Z","iopub.status.idle":"2022-04-18T12:17:22.843269Z","shell.execute_reply.started":"2022-04-18T12:17:22.839648Z","shell.execute_reply":"2022-04-18T12:17:22.842389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [2] Logistic Regression 사용한 모델학습\n#model = LogisticRegression(solver='newton-cg')\n#model.fit(trainX, trainY)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:22.844777Z","iopub.execute_input":"2022-04-18T12:17:22.845275Z","iopub.status.idle":"2022-04-18T12:17:22.86206Z","shell.execute_reply.started":"2022-04-18T12:17:22.84522Z","shell.execute_reply":"2022-04-18T12:17:22.86102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [3] LinearDiscriminantAnalysis 사용한 모델학습\n#model = LinearDiscriminantAnalysis()\n#model.fit(trainX, trainY)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:22.863422Z","iopub.execute_input":"2022-04-18T12:17:22.863992Z","iopub.status.idle":"2022-04-18T12:17:22.876076Z","shell.execute_reply.started":"2022-04-18T12:17:22.863951Z","shell.execute_reply":"2022-04-18T12:17:22.875221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [4] QuadraticDiscriminantAnalysis\n#model = QuadraticDiscriminantAnalysis()\n#model.fit(trainX, trainY)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:22.87721Z","iopub.execute_input":"2022-04-18T12:17:22.878065Z","iopub.status.idle":"2022-04-18T12:17:22.887144Z","shell.execute_reply.started":"2022-04-18T12:17:22.878004Z","shell.execute_reply":"2022-04-18T12:17:22.886392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [5] DecisionTreeClassifier 사용한 모델학습\n#model = DecisionTreeClassifier()\n#model.fit(trainX, trainY)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:22.888362Z","iopub.execute_input":"2022-04-18T12:17:22.889171Z","iopub.status.idle":"2022-04-18T12:17:22.901799Z","shell.execute_reply.started":"2022-04-18T12:17:22.88912Z","shell.execute_reply":"2022-04-18T12:17:22.901145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [6] RandomForestClassifier 사용한 모델학습\n#model = RandomForestClassifier()\n#model.fit(trainX, trainY)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:22.903265Z","iopub.execute_input":"2022-04-18T12:17:22.903752Z","iopub.status.idle":"2022-04-18T12:17:23.298086Z","shell.execute_reply.started":"2022-04-18T12:17:22.903719Z","shell.execute_reply":"2022-04-18T12:17:23.297268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test 데이터의 예측값 구하기\ntest_pred = model.predict(testX)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:23.300574Z","iopub.execute_input":"2022-04-18T12:17:23.301185Z","iopub.status.idle":"2022-04-18T12:17:23.331063Z","shell.execute_reply.started":"2022-04-18T12:17:23.301139Z","shell.execute_reply":"2022-04-18T12:17:23.330109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit['label'] = test_pred\nsubmit.to_csv('result_e1p2', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:17:23.332364Z","iopub.execute_input":"2022-04-18T12:17:23.332675Z","iopub.status.idle":"2022-04-18T12:17:23.341632Z","shell.execute_reply.started":"2022-04-18T12:17:23.332634Z","shell.execute_reply":"2022-04-18T12:17:23.34065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"손글씨 숫자 인식 문제는 분류 문제입니다. \n\n다음과 같은 여섯가지 방법론을 사용해 보았다.\n\n1) KNeighborsClassifier : 이 문제에서 가장 높은 성능을 보였다.\n\n2) LogisticRegression\n\n3) LinearDiscriminantAnalysis : LDA는 QDA보다 성능이 훨씬 괜찮았다.\n\n4) QuadraticDiscriminantAnalysis : 가장 안좋은 성능을 보였다.\n\n5) DecisionTreeClassifier\n\n6) RandomForestClassifier : 두번째로 좋은 성능을 보였다.\n\n처음에 index 열을 제거해주지 않았더니 성능이 좋지않았다. \n\n모델학습에 필요하지 않은 데이터들은 없애주는 것이 효과적이다.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}