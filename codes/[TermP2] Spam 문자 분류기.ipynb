{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e3d4c7",
   "metadata": {
    "papermill": {
     "duration": 0.026978,
     "end_time": "2022-06-20T13:29:11.288404",
     "exception": false,
     "start_time": "2022-06-20T13:29:11.261426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 반드시 처음부터 끝까지 스켈레톤 코드를 살펴보고 구현하기 시작하길 바란다\n",
    "\n",
    "## 1. 스켈레톤 코드를 [복사 및 편집] 하여 사용한다.\n",
    "## 2. 아래의 [Empty Module 3개]를 직접 구현한다.\n",
    "## (필수) 코드 작성 전에 Overview의 Description을 읽고, 본 프로젝트의 방향성을 이해하고 시작하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0031b71c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:11.331560Z",
     "iopub.status.busy": "2022-06-20T13:29:11.331038Z",
     "iopub.status.idle": "2022-06-20T13:29:11.347371Z",
     "shell.execute_reply": "2022-06-20T13:29:11.346556Z"
    },
    "papermill": {
     "duration": 0.040582,
     "end_time": "2022-06-20T13:29:11.349503",
     "exception": false,
     "start_time": "2022-06-20T13:29:11.308921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        os.path.join(dirname, filename)\n",
    "        #print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99b886a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:11.393029Z",
     "iopub.status.busy": "2022-06-20T13:29:11.391939Z",
     "iopub.status.idle": "2022-06-20T13:29:13.104105Z",
     "shell.execute_reply": "2022-06-20T13:29:13.103230Z"
    },
    "papermill": {
     "duration": 1.736821,
     "end_time": "2022-06-20T13:29:13.106821",
     "exception": false,
     "start_time": "2022-06-20T13:29:11.370000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 본 프로젝트를 위한 패키지 로드\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e6810",
   "metadata": {
    "papermill": {
     "duration": 0.019864,
     "end_time": "2022-06-20T13:29:13.149476",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.129612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 자연어 처리를 위한 라이브러리 다운로드\n",
    "- nltk 라이브러리에서 punkt 데이터를 다운 받음, 이 데이터는 영화 리뷰와 같은 문서 데이터로 문자의 tokeninizer를 위해서 필요하다\n",
    "- nltk 라이브러리를 이용해서 불용어를 다운 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e48b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:13.191362Z",
     "iopub.status.busy": "2022-06-20T13:29:13.191061Z",
     "iopub.status.idle": "2022-06-20T13:29:13.416877Z",
     "shell.execute_reply": "2022-06-20T13:29:13.415682Z"
    },
    "papermill": {
     "duration": 0.2508,
     "end_time": "2022-06-20T13:29:13.420417",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.169617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 자연어 처리를 위한 라이브러리 다운로드\n",
    "## nltk 라이브러리 - punkt 데이터 : 영화 리뷰와 같은 문서 데이터로 문자의 tokeninizer에 필요\n",
    "## nltk 라이브러리 - 불용어\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15de1ae",
   "metadata": {
    "papermill": {
     "duration": 0.021098,
     "end_time": "2022-06-20T13:29:13.462968",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.441870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b9fb57a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:13.506646Z",
     "iopub.status.busy": "2022-06-20T13:29:13.506024Z",
     "iopub.status.idle": "2022-06-20T13:29:13.551968Z",
     "shell.execute_reply": "2022-06-20T13:29:13.551129Z"
    },
    "papermill": {
     "duration": 0.070571,
     "end_time": "2022-06-20T13:29:13.554281",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.483710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/2022-ml-project2/train.csv\",encoding=\"latin-1\")\n",
    "df_test = pd.read_csv(\"../input/2022-ml-project2/test.csv\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b3c5b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:13.597501Z",
     "iopub.status.busy": "2022-06-20T13:29:13.597144Z",
     "iopub.status.idle": "2022-06-20T13:29:13.614404Z",
     "shell.execute_reply": "2022-06-20T13:29:13.613843Z"
    },
    "papermill": {
     "duration": 0.041381,
     "end_time": "2022-06-20T13:29:13.616455",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.575074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Data</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>No I'm in the same boat. Still here at my moms...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(Bank of Granite issues Strong-Buy) EXPLOSIVE ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>They r giving a second chance to rahul dengra.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>O i played smash bros  &amp;lt;#&amp;gt;  religiously.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PRIVATE! Your 2003 Account Statement for 07973...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Data Class\n",
       "0           0  No I'm in the same boat. Still here at my moms...   ham\n",
       "1           1  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...  spam\n",
       "2           2     They r giving a second chance to rahul dengra.   ham\n",
       "3           3     O i played smash bros  &lt;#&gt;  religiously.   ham\n",
       "4           4  PRIVATE! Your 2003 Account Statement for 07973...  spam"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93385700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:13.661414Z",
     "iopub.status.busy": "2022-06-20T13:29:13.660833Z",
     "iopub.status.idle": "2022-06-20T13:29:13.668915Z",
     "shell.execute_reply": "2022-06-20T13:29:13.668128Z"
    },
    "papermill": {
     "duration": 0.033667,
     "end_time": "2022-06-20T13:29:13.671215",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.637548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = df_train[\"Data\"]\n",
    "y_train = df_train[\"Class\"]\n",
    "X_test = df_test[\"Data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a44fb",
   "metadata": {
    "papermill": {
     "duration": 0.02329,
     "end_time": "2022-06-20T13:29:13.715742",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.692452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Empty Module #1] 텍스트 데이터 전처리  \n",
    "\n",
    "목표: 텍스트 데이터를 처리하기 위한 여러 과정을 거쳐, 머신을 위한 데이터를 만든다. \n",
    "\n",
    "```\n",
    "[input]\n",
    "--------------\n",
    "- text: 텍스트 문장 데이터 \n",
    "\n",
    "[output]\n",
    "--------------\n",
    "- text: 전처리를 완료한 문장 데이터 \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e5e498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:13.761375Z",
     "iopub.status.busy": "2022-06-20T13:29:13.760740Z",
     "iopub.status.idle": "2022-06-20T13:29:13.770749Z",
     "shell.execute_reply": "2022-06-20T13:29:13.770074Z"
    },
    "papermill": {
     "duration": 0.03512,
     "end_time": "2022-06-20T13:29:13.773083",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.737963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #1] 텍스트 데이터 전처리\n",
    "# ------------------------------------------------\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def data_processing(text):\n",
    "    # ------------------------------------------------------------\n",
    "    # 구현 가이드라인 \n",
    "    # ------------------------------------------------------------\n",
    "    # [1] re.sub 사용해 text 속 '[^A-Za-z]' 외의 문자만을 찾아내 제거한후, pre_words 변수에 저장\n",
    "    #      1) pattern은 '[^A-Za-z]', repl=' ' 로 각각 설정.\n",
    "    #      2) 이모지나 숫자,점과 같은 문자외의 것들을 제거했다. (이모지는 감정 분석과 관련해서 몇가지 의미를 나타내지만 이 테스크에서는 이러한 의미도 제거함.)\n",
    "    #\n",
    "    # [2] pre_words의 lower 내장 함수를 이용해 대문자들은 소문자로 변경\n",
    "    #      1)  대, 소문자가 구분되어 있으면 \"Go\"와 \"go\" 와 같이 동일한 단어를 머신은 다른 단어로 취급한다. 따라서 대문자를 모두 소문자로 변경.\n",
    "    #\n",
    "    # [3] word_tokenize 함수를 이용해 pre_word 를 토큰화하여 word를 리스트화한 후 tokenized_words변수에 저장\n",
    "    #\n",
    "    # [4] nltk 라이브러리로 다운 받은 stopwords의 \"words\" 내장 함수를 이용해 english 불용어를 찾아서 stops 변수에 저장  \n",
    "    #      1) 불용어: 텍스트 분류에서 불용어는 텍스트의 중요도을 결정하는데 영향을 미치지 않는 단어임. \n",
    "    #                    (ex: the, we, a , will), 따라서 불필요한 단어가 예측 모델에 악영향을 끼칠 수 있기 때문에 제거.\n",
    "    #\n",
    "    # [5] [3] 에서 찾은 문자열 중 단어가 [4] 에서 찾은 불용어 속에 없을 경우, tokenized_words_remove 리스트에 append \n",
    "    #\n",
    "    # [6] tokenized_words_remove의 단어를 PorterStemmer 속 stem 내장 함수를 이용해, 동일 의미를 갖는 단어를 동일한 단어로 변경하는 과정을 거친 후 다시 저장.\n",
    "    #    \n",
    "    # ------------------------------------------------------------\n",
    "    ##############\n",
    "    # [1]\n",
    "    pre_words = re.sub('[^A-Za-z]',' ', text)\n",
    "    ##############\n",
    "    # [2]\n",
    "    pre_words = pre_words.lower()\n",
    "    ##############\n",
    "    # [3]\n",
    "    tokenized_words = word_tokenize(pre_words)\n",
    "    ##############\n",
    "    # [4]\n",
    "    stops = stopwords.words('english')\n",
    "    ##############\n",
    "    \n",
    "    ## tokenized_words_remove = [word for word in tokenized_words if word not in stops]\n",
    "    tokenized_words_remove=[]\n",
    "    for w in tokenized_words: \n",
    "        # [5]\n",
    "        ##############################################\n",
    "        if(w not in stops):\n",
    "             tokenized_words_remove.append(w)\n",
    "        ##############################################\n",
    "\n",
    "    ## stemming : 어간추출, tokenized_words_remove 어간 추출 시행\n",
    "    ## tokenized_words_remove = [stemmer.stem(word) for word in tokenized_words_remove]\n",
    "    stemmer = PorterStemmer()\n",
    "    for i in range(len(tokenized_words_remove)):\n",
    "        # [6]\n",
    "        ##############################################\n",
    "        tokenized_words_remove[i] = (stemmer.stem(tokenized_words_remove[i]))\n",
    "        ##############################################\n",
    "    \n",
    "    return ( \" \".join( tokenized_words_remove ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe9a635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:13.818469Z",
     "iopub.status.busy": "2022-06-20T13:29:13.818001Z",
     "iopub.status.idle": "2022-06-20T13:29:17.654323Z",
     "shell.execute_reply": "2022-06-20T13:29:17.653374Z"
    },
    "papermill": {
     "duration": 3.861719,
     "end_time": "2022-06-20T13:29:17.656800",
     "exception": false,
     "start_time": "2022-06-20T13:29:13.795081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = [data_processing(i) for i in X_train]\n",
    "X_test = [data_processing(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606a50b",
   "metadata": {
    "papermill": {
     "duration": 0.020683,
     "end_time": "2022-06-20T13:29:17.699192",
     "exception": false,
     "start_time": "2022-06-20T13:29:17.678509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Empty Module #2] Bag of Word \n",
    "\n",
    "목표: 문장 데이터를 머신을 학습하기 위한 실수화된 Feature로 변경하기로한다. \n",
    "\n",
    "- train 과 test 데이터 전부 type을 ('U')로 변경하여 Countvectorizer를 사용한다. \n",
    "- [설명 링크](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39cc9d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:17.743822Z",
     "iopub.status.busy": "2022-06-20T13:29:17.743413Z",
     "iopub.status.idle": "2022-06-20T13:29:18.044125Z",
     "shell.execute_reply": "2022-06-20T13:29:18.043282Z"
    },
    "papermill": {
     "duration": 0.326388,
     "end_time": "2022-06-20T13:29:18.047221",
     "exception": false,
     "start_time": "2022-06-20T13:29:17.720833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Bag of Word : 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "## 문장 데이터를 머신을 학습하기 위한 실수화된 Feature로 변경하는 과정\n",
    "\n",
    "# ------------------------------------------------\n",
    "# [Empty Module #2] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1] CountVectorizer를 정의\n",
    "#           1) max_features를 100으로 지정 \n",
    "#\n",
    "# [2] X_train 과 X_test를 numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "#\n",
    "# [3] CountVectorizer를 이용해 X_train은 학습 및 변환(fit_transform)을 하고, X_test는 변환(transform)을 진행. \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# \n",
    "##############\n",
    "# [1] BaseLine 별 vectorizer \n",
    "# BaseLine_3\n",
    "vectorizer_count = CountVectorizer(max_features=100)\n",
    "\n",
    "# Baseline_2(TfidVectorizer)_v2\n",
    "vectorizer_tfid = TfidfVectorizer(max_features=100)\n",
    "\n",
    "# Baseline_1(HashingVectorizer)\n",
    "vectorizer_hash = HashingVectorizer()\n",
    "\n",
    "########\n",
    "# [2]\n",
    "## # x_train 과 x_test를 numpy array로 변환 후 데이터 타입을 \"U : unicode 문자\"로 변경해 저장\n",
    "##############################################\n",
    "X_train = np.array(X_train, dtype='U')\n",
    "X_test = np.array(X_test, dtype='U')\n",
    "##############################################\n",
    "\n",
    "##############\n",
    "# [3]\n",
    "X_train_features_count = vectorizer_count.fit_transform(X_train)\n",
    "X_test_features_count = vectorizer_count.transform(X_test)\n",
    "\n",
    "X_train_features_tfid = vectorizer_tfid.fit_transform(X_train)\n",
    "X_test_features_tfid = vectorizer_tfid.transform(X_test)\n",
    "\n",
    "X_train_features_hash = vectorizer_hash.fit_transform(X_train)\n",
    "X_test_features_hash = vectorizer_hash.transform(X_test)\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dca56b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:18.091817Z",
     "iopub.status.busy": "2022-06-20T13:29:18.091387Z",
     "iopub.status.idle": "2022-06-20T13:29:18.105862Z",
     "shell.execute_reply": "2022-06-20T13:29:18.105053Z"
    },
    "papermill": {
     "duration": 0.039653,
     "end_time": "2022-06-20T13:29:18.108369",
     "exception": false,
     "start_time": "2022-06-20T13:29:18.068716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "## spam인 경우 1로, ham인 경우 0으로 변경\n",
    "## unit8 : 부호 없는 8비트 정수형\n",
    "## SettingWithCopyWarning: 오류X, 단순 객체 복제를 이용해서 경고 발생 -> .copy() 이용시 해결\n",
    "\n",
    "#ham의 경우 0으로 지정하고, spam의 경우에는 1로 라벨을 변경해줌\n",
    "y_train[y_train==\"ham\"] = 0\n",
    "y_train[y_train==\"spam\"] = 1\n",
    "y_train = y_train.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d468e91c",
   "metadata": {
    "papermill": {
     "duration": 0.022479,
     "end_time": "2022-06-20T13:29:18.152538",
     "exception": false,
     "start_time": "2022-06-20T13:29:18.130059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## [Empty Module #3] SVM: classifier\n",
    "목표: SVC() 를 활용해 classification 을 진행\n",
    "\n",
    "- fit()으로 train 에 대한 머신러닝 학습\n",
    "- predict()으로 test 에 대한 정답을 추론 하여 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b6ed04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:18.197876Z",
     "iopub.status.busy": "2022-06-20T13:29:18.197357Z",
     "iopub.status.idle": "2022-06-20T13:29:18.473996Z",
     "shell.execute_reply": "2022-06-20T13:29:18.473174Z"
    },
    "papermill": {
     "duration": 0.30221,
     "end_time": "2022-06-20T13:29:18.476422",
     "exception": false,
     "start_time": "2022-06-20T13:29:18.174212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #3] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.svm import SVC\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1]  SVC 선언 (베이스라인 에서 gamma=\"auto\" 사용 )\n",
    "#\n",
    "# [2] X_train_features과 y_train으로 SVC 학습진행 후, X_test_features로 predict 진행\n",
    "#\n",
    "# [3] y_pred에 predict한 결과값 저장\n",
    "# ------------------------------------------------------------\n",
    "###########\n",
    "# [1]\n",
    "svc=SVC(gamma='auto')\n",
    "##############\n",
    "\n",
    "# [2]\n",
    "##############################################\n",
    "svc.fit(X_train_features_count, y_train)\n",
    "##############################################\n",
    "\n",
    "##############\n",
    "# [3]\n",
    "y_pred=svc.predict(X_test_features_count)\n",
    "##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e79ad5",
   "metadata": {
    "papermill": {
     "duration": 0.021297,
     "end_time": "2022-06-20T13:29:18.519393",
     "exception": false,
     "start_time": "2022-06-20T13:29:18.498096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "489bc8fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:18.563979Z",
     "iopub.status.busy": "2022-06-20T13:29:18.563680Z",
     "iopub.status.idle": "2022-06-20T13:29:18.581851Z",
     "shell.execute_reply": "2022-06-20T13:29:18.581175Z"
    },
    "papermill": {
     "duration": 0.042949,
     "end_time": "2022-06-20T13:29:18.583990",
     "exception": false,
     "start_time": "2022-06-20T13:29:18.541041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## submit의 Class에 저장하여 제출\n",
    "\n",
    "submit = pd.read_csv('../input/2022-ml-project2/sample_submit.csv')\n",
    "submit['Class'] = y_pred\n",
    "submit.to_csv('CountVectorizer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e4d80",
   "metadata": {
    "papermill": {
     "duration": 0.020981,
     "end_time": "2022-06-20T13:29:18.627055",
     "exception": false,
     "start_time": "2022-06-20T13:29:18.606074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## BaseLine 2 (TfidVectorizer) 와 BaseLine 1 ( HashingVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b98948a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T13:29:18.671648Z",
     "iopub.status.busy": "2022-06-20T13:29:18.671128Z",
     "iopub.status.idle": "2022-06-20T13:29:19.854745Z",
     "shell.execute_reply": "2022-06-20T13:29:19.853812Z"
    },
    "papermill": {
     "duration": 1.208972,
     "end_time": "2022-06-20T13:29:19.857470",
     "exception": false,
     "start_time": "2022-06-20T13:29:18.648498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TfidVectorizer화 한 데이터를 학습시킨 결과\n",
    "svc.fit(X_train_features_tfid, y_train)\n",
    "y_pred=svc.predict(X_test_features_tfid)\n",
    "submit['Class'] = y_pred\n",
    "submit.to_csv('TfidVectorizer.csv', index=False)\n",
    "\n",
    "\n",
    "## HashingVectorizer화 한 데이터를 학습시킨 결과\n",
    "svc.fit(X_train_features_hash, y_train)\n",
    "y_pred=svc.predict(X_test_features_hash)\n",
    "submit['Class'] = y_pred\n",
    "submit.to_csv('HashingVectorizer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc598ea",
   "metadata": {
    "papermill": {
     "duration": 0.021667,
     "end_time": "2022-06-20T13:29:19.901225",
     "exception": false,
     "start_time": "2022-06-20T13:29:19.879558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.513733,
   "end_time": "2022-06-20T13:29:20.647148",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-20T13:29:01.133415",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
