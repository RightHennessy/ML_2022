# Decision Tree

## 의사 결정 나무

</br>

[노션 보러가기](https://www.notion.so/Decision-Tree-9b5118232aa745a89a18e7c8a2543123)

</br>

### 정의

학습데이터를 분석하여 데이터에 내재되어 있는 패턴을 통해 새롭게 관측된 데이터를 예측 및 분류하는 모델

목적(Y)과 자료(X)에 따라 적절한 분리 기준과 정지 규칙을 지정하여 의사 결정 나무를 생성

</br>

### 장점

✔ 이해하기 쉽고 적용하기 쉽다. - 나무구조 (If-then 규칙)

✔ 의사 결정 과정에 대한 설명(해석)이 가능하다.

✔ 중요한 변수 선택에 유용하다. 

✔ 데이터의 통계적 가정이 필요없다.  

</br>

### 단점

✔ 좋은 모형을 만들기 위해 많은 데이터가 필요하다. 

✔ 모형을 만드는데 (Tree building) 상대적으로 시간이 많이 소요된다. 

✔ 데이터의 변화에 민감하다. → 학습과 테스트 데이터의 도메인이 유사해야한다. 

✔ 선형 구조형 데이터 예측 시 더 복잡하다. 

</br>

### 데이터 분석

1) 데이터 : 다변량 변수 사용

2) 모델 학습 - 트리구조 이용

    한번에 설명 변수 하나씩 데이터를 선택한다.

    2개 혹은 그 이상의 부분집합으로 분할한다. 

    데이터 순도가 균일해지도록 재귀적 분할한다. : Recursive Patitioning

    → 분류 : 끝 노드에 비슷한 범주(클래스)를 갖고 있는 관측 데이터끼리

    → 예측 : 끝 노드에 비슷한 수치(연속된 값)를 갖고 있는 관측 데이터끼리 

3) 추론 (판별) 

    → 분류 : 끝 노드에서 가장 빈도가 높은 종속변수(y)를 새로운 데이터에 부여 

    → 회귀 : 끝 노드의 종속변수(y)의 평균을 예측 값으로 반환

</br>

### 재귀적 분할 알고리즘과 불순도 알고리즘

✔ 재귀적 분할 알고리즘 

 CART : Classification And Regression Tree

 C4.5 , C5.0

 CHAID : Chi-square Automatic Interaction Detection

 </br>
 

✔ 불순도 알고리즘 → 분할 기준

 지니 지수 : Gini index

 엔트로피 지수 : Entropy index

 정보 이익 : Information Gain, 정보 이익률 :  Information gain ratio

 카이제곱 통계량 : Chi-square Statistic

![image](https://user-images.githubusercontent.com/88828858/182009581-78dc65f9-7fc7-4e45-b867-70091f5bc6ca.png)


[예시](https://github.com/sejongresearch/2022.MachineLearning/blob/main/LectureNote/%5B%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%5D%5B7%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%5D%20%E1%84%8B%E1%85%B4%E1%84%89%E1%85%A1%E1%84%80%E1%85%A7%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%82%E1%85%A1%E1%84%86%E1%85%AE_%E1%84%8B%E1%85%B5%E1%84%85%E1%85%A9%E1%86%AB.pdf)

</br>

### 분류나무 : Classification Tree

목표 변수가 범주형 변수 → 분류

✔ 분류 알고리즘과 불순도 지표  

CART : 지니 지수 

C4.5 : 엔트로피, 정보 이익, 정보 이익 비율 C5.0 

CHAID : 카이 제곱 통계량 

✔ 분류 결과 : 소속 집단 판단, 경향성도 확률로 표현 가능

</br>

### 회귀 나무 : Regression Tree

목표 변수가 수치형 변수 → 예측

✔ 회귀 알고리즘과 불순도 지표 

CART : F 통계량과 분산 감소량(= |실제값 - 예측값|² ) → 실제 값과 예측값의 평균 차이가 작도록 !

불순도 측정 방법 : 제곱오차 합, SSE = sum[yi - yi^]² (i=1부터 n) 

✔ 성능 평가 방법 : 예측 모델 평가 방법 - RMSE

✔ 회귀 결과 : 끝 마디 집단의 평균, 예측일 경우 ~~회귀 나무~~ 보다 신경망 또는 회귀 분석이 더 좋다.

</br>

### 분류나무의 과적합 방지

끝없는 분할의 단점 → 과적합 (Overfitting)

✔ 성장 멈추기 : Stop Condition

나무 모델의 깊이 파라미터로 설정 : depth

나무 모델을 성장 시키면서 특정 조건에서 성장 중단

노드 내의 최소 관측치의 수 설정

불순도 최소 감소량 : △t (엔트로피의 차이) < α → CHAID에서 사용, 가지치기 없이 종료 

✔ 가지치기 : Pruning

완전 모형 생성 후 가지치기

데이터를 버리는 것이 아닌 합치는 개념 !

성장 멈추기보다 성능 우수하다 !

가지치기 비용함수를 최소로 하는 분기를 찾는다

</br>

### 앙상블 : Ensemble

여러 모델을 함께 사용 : 의사결정나무, KNN, 로지스틱 등

설명보다는 예측이 중요할 경우 사용 → 예측 알고리즘을 조합하여 예측 성능을 향상

랜덤 숲 (Random Forest), Boosted Trees → 좋은 의사 결정 나무를 모아서 숲을 만들자 !

✔ Random Forest  → 최근 의사결정 방법론으로 가장 많이 사용됨

Bootstrap 사용 : 데이터로부터 복원 추출(뽑은 표본 원상복귀)을 이용하여 여러샘플을 추출

Forest 생성 : 무작위로 예측 변수를 선택하여 모델 구축 

(의사결정 나무는 예측 변수 선택 시 기준 지표를 사용했으나 무작위 숲에서는 무작위)

나무 구조이지만, 숲이 되면서 해석가능한 모델의 장점 사라짐

중요한 변수 판별은 가능
